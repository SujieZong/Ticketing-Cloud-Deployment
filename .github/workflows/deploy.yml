name: Deploy Ticketing System

on:
  workflow_dispatch:
    inputs:
      action:
        description: "Deployment action"
        required: true
        type: choice
        options:
          - "infrastructure-only"
          - "services-only"
          - "full-deployment"
          - "destroy-infrastructure"
          - "force-cleanup"

env:
  AWS_REGION: us-west-2
  TF_VERSION: 1.6.0

jobs:
  # Job 1: Build Services
  build:
    name: Build Java Services
    runs-on: ubuntu-latest
    if: github.event.inputs.action == 'services-only' || github.event.inputs.action == 'full-deployment'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up JDK 21
        uses: actions/setup-java@v4
        with:
          java-version: "21"
          distribution: "temurin"
          cache: maven

      - name: Build PurchaseService
        working-directory: ./PurchaseService
        run: |
          echo "ðŸ”¨ Building PurchaseService..."
          mvn clean package -DskipTests

      - name: Build QueryService
        working-directory: ./QueryService
        run: |
          echo "ðŸ”¨ Building QueryService..."
          mvn clean package -DskipTests

      - name: Build RabbitCombinedConsumer
        working-directory: ./RabbitCombinedConsumer
        run: |
          echo "ðŸ”¨ Building RabbitCombinedConsumer..."
          mvn clean package -DskipTests

      - name: Run Tests
        run: |
          echo "ðŸ§ª Running unit tests..."
          cd PurchaseService && mvn test || true
          cd ../QueryService && mvn test || true
          cd ../RabbitCombinedConsumer && mvn test || true

      - name: Upload Build Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: service-jars
          path: |
            PurchaseService/target/*.jar
            QueryService/target/*.jar
            RabbitCombinedConsumer/target/*.jar
          retention-days: 1

  # Job 2: Force Cleanup (No Terraform State Required)
  force-cleanup:
    name: Force Cleanup AWS Resources
    runs-on: ubuntu-latest
    if: github.event.inputs.action == 'force-cleanup'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-session-token: ${{ secrets.AWS_SESSION_TOKEN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Run Cleanup Script
        run: |
          echo "ðŸ§¹ Running force cleanup (no Terraform state needed)..."
          chmod +x ./config/scripts/cleanup-aws-resources.sh
          ./config/scripts/cleanup-aws-resources.sh

      - name: Cleanup Summary
        run: |
          echo "### ðŸ§¹ Force Cleanup Complete!" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "All AWS resources have been deleted using the cleanup script." >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Next Steps:**" >> $GITHUB_STEP_SUMMARY
          echo "1. Clear GitHub Actions cache: https://github.com/${{ github.repository }}/actions/caches" >> $GITHUB_STEP_SUMMARY
          echo "2. Wait 2-3 minutes for AWS to fully process deletions" >> $GITHUB_STEP_SUMMARY
          echo "3. Run 'full-deployment' to deploy fresh infrastructure" >> $GITHUB_STEP_SUMMARY

  # Job 3: Terraform Infrastructure
  terraform-infrastructure:
    name: Deploy Infrastructure with Terraform
    runs-on: ubuntu-latest
    if: github.event.inputs.action == 'infrastructure-only' || github.event.inputs.action == 'full-deployment' || github.event.inputs.action == 'destroy-infrastructure'

    outputs:
      ecr_registry: ${{ steps.tf-outputs.outputs.ecr_registry }}
      ecs_cluster: ${{ steps.tf-outputs.outputs.ecs_cluster }}
      alb_dns: ${{ steps.tf-outputs.outputs.alb_dns }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Clear stale caches (for AWS Learner Lab)
        continue-on-error: true
        run: |
          echo "ðŸ§¹ Note: Clear GitHub Actions caches manually if you see 'resource already exists' errors"
          echo "Go to: https://github.com/${{ github.repository }}/actions/caches"

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-session-token: ${{ secrets.AWS_SESSION_TOKEN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TF_VERSION }}
          terraform_wrapper: false

      # Restore Terraform state for destroy operations only
      - name: Restore Terraform State
        if: github.event.inputs.action == 'destroy-infrastructure'
        uses: actions/cache/restore@v3
        with:
          path: |
            config/terraform/terraform.tfstate
            config/terraform/terraform.tfstate.backup
            config/terraform/.terraform
          key: terraform-state-${{ github.ref_name }}
          restore-keys: |
            terraform-state-${{ github.ref_name }}
            terraform-state-

      - name: Create terraform.tfvars
        working-directory: ./config/terraform
        run: |
          cat > terraform.tfvars <<EOF
          aws_region     = "${{ env.AWS_REGION }}"
          aws_account_id = "${{ secrets.AWS_ACCOUNT_ID }}"
          EOF

      - name: Terraform Init
        working-directory: ./config/terraform
        run: terraform init -upgrade

      - name: Terraform Plan
        working-directory: ./config/terraform
        run: terraform plan -no-color
        if: github.event.inputs.action != 'destroy-infrastructure'

      - name: Terraform Apply
        working-directory: ./config/terraform
        run: terraform apply -auto-approve
        if: github.event.inputs.action != 'destroy-infrastructure'

      # Save state AFTER successful apply for future destroy operations
      - name: Save Terraform State
        if: github.event.inputs.action != 'destroy-infrastructure' && success()
        uses: actions/cache/save@v3
        with:
          path: |
            config/terraform/terraform.tfstate
            config/terraform/terraform.tfstate.backup
            config/terraform/.terraform
          key: terraform-state-${{ github.ref_name }}-${{ github.run_number }}

      - name: Terraform Destroy
        id: tf-destroy
        working-directory: ./config/terraform
        continue-on-error: true
        run: terraform destroy -auto-approve
        if: github.event.inputs.action == 'destroy-infrastructure'

      # Fallback: Use cleanup script if Terraform destroy fails (no state available)
      - name: Fallback - Manual Cleanup
        if: github.event.inputs.action == 'destroy-infrastructure' && steps.tf-destroy.outcome == 'failure'
        run: |
          echo "âš ï¸ Terraform destroy failed (likely no state found)"
          echo "ðŸ§¹ Running manual cleanup script instead..."
          chmod +x ./config/scripts/cleanup-aws-resources.sh
          ./config/scripts/cleanup-aws-resources.sh

      # Delete state cache after destroy
      - name: Clear State Cache After Destroy
        if: github.event.inputs.action == 'destroy-infrastructure' && success()
        continue-on-error: true
        run: |
          echo "âœ… Resources destroyed. State cache will expire naturally."
          echo "ðŸ§¹ To clear cache manually: https://github.com/${{ github.repository }}/actions/caches"

      - name: Export Infrastructure Outputs
        id: tf-outputs
        working-directory: ./config/terraform
        if: github.event.inputs.action != 'destroy-infrastructure'
        run: |
          ECR_REGISTRY=$(terraform output -raw ecr_registry 2>/dev/null || echo "not-available")
          ECS_CLUSTER=$(terraform output -raw ecs_cluster_name 2>/dev/null || echo "ticketing-prod-cluster")
          ALB_DNS=$(terraform output -raw alb_dns_name 2>/dev/null || echo "not-available")

          echo "ecr_registry=$ECR_REGISTRY" >> $GITHUB_OUTPUT
          echo "ecs_cluster=$ECS_CLUSTER" >> $GITHUB_OUTPUT
          echo "alb_dns=$ALB_DNS" >> $GITHUB_OUTPUT

      - name: Infrastructure Summary
        if: github.event.inputs.action != 'destroy-infrastructure'
        run: |
          echo "### ðŸ—ï¸ Infrastructure Deployed!" >> $GITHUB_STEP_SUMMARY
          echo "**ECS Cluster:** ${{ steps.tf-outputs.outputs.ecs_cluster }}" >> $GITHUB_STEP_SUMMARY
          echo "**ALB URL:** http://${{ steps.tf-outputs.outputs.alb_dns }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Next Step:** Run 'services-only' deployment to deploy application code" >> $GITHUB_STEP_SUMMARY

      - name: Destroy Summary
        if: github.event.inputs.action == 'destroy-infrastructure'
        run: |
          echo "### ðŸ—‘ï¸ Infrastructure Destroyed!" >> $GITHUB_STEP_SUMMARY
          echo "All AWS resources have been removed." >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Next Step:** Run 'full-deployment' to recreate infrastructure" >> $GITHUB_STEP_SUMMARY

  # Job 4: Deploy Services
  deploy-services:
    name: Build & Deploy Docker Services
    runs-on: ubuntu-latest
    needs: [build, terraform-infrastructure]
    if: |
      always() && 
      (needs.build.result == 'success' || needs.build.result == 'skipped') &&
      (needs.terraform-infrastructure.result == 'success' || needs.terraform-infrastructure.result == 'skipped') &&
      (github.event.inputs.action == 'services-only' || github.event.inputs.action == 'full-deployment')

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download Build Artifacts
        uses: actions/download-artifact@v4
        with:
          name: service-jars

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-session-token: ${{ secrets.AWS_SESSION_TOKEN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build and push PurchaseService
        env:
          ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
          IMAGE_TAG: ${{ github.sha }}
        run: |
          echo "ðŸ³ Building and pushing PurchaseService..."
          cd PurchaseService
          docker buildx build --platform linux/amd64 --push \
            -t $ECR_REGISTRY/ticketing-purchase-service:$IMAGE_TAG \
            -t $ECR_REGISTRY/ticketing-purchase-service:latest \
            .

      - name: Build and push QueryService
        env:
          ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
          IMAGE_TAG: ${{ github.sha }}
        run: |
          echo "ðŸ³ Building and pushing QueryService..."
          cd QueryService
          docker buildx build --platform linux/amd64 --push \
            -t $ECR_REGISTRY/ticketing-query-service:$IMAGE_TAG \
            -t $ECR_REGISTRY/ticketing-query-service:latest \
            .

      - name: Build and push MQ Consumer
        env:
          ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
          IMAGE_TAG: ${{ github.sha }}
        run: |
          echo "ðŸ³ Building and pushing RabbitCombinedConsumer..."
          cd RabbitCombinedConsumer
          docker buildx build --platform linux/amd64 --push \
            -t $ECR_REGISTRY/ticketing-mq-projection-service:$IMAGE_TAG \
            -t $ECR_REGISTRY/ticketing-mq-projection-service:latest \
            .

      - name: Force ECS service redeployment
        run: |
          echo "â™»ï¸ Updating ECS services with new images..."

          aws ecs update-service \
            --cluster ticketing-prod-cluster \
            --service ticketing-purchase-service \
            --force-new-deployment \
            --region ${{ env.AWS_REGION }} || echo "âš ï¸ Failed to update purchase-service (may not exist yet)"

          aws ecs update-service \
            --cluster ticketing-prod-cluster \
            --service ticketing-query-service \
            --force-new-deployment \
            --region ${{ env.AWS_REGION }} || echo "âš ï¸ Failed to update query-service (may not exist yet)"

          aws ecs update-service \
            --cluster ticketing-prod-cluster \
            --service ticketing-mq-projection-service \
            --force-new-deployment \
            --region ${{ env.AWS_REGION }} || echo "âš ï¸ Failed to update mq-projection-service (may not exist yet)"

      - name: Wait for deployment
        run: |
          echo "â³ Waiting for services to stabilize (2 minutes)..."
          sleep 120

      - name: Get ALB URL
        id: get-alb
        run: |
          # Try to get ALB DNS from Terraform output
          if [ -f "config/terraform/terraform.tfstate" ]; then
            cd config/terraform
            ALB_DNS=$(terraform output -raw alb_dns_name 2>/dev/null || echo "not-available")
          else
            # Try to get from AWS directly
            ALB_DNS=$(aws elbv2 describe-load-balancers \
              --query "LoadBalancers[?contains(LoadBalancerName, 'ticketing')].DNSName | [0]" \
              --output text 2>/dev/null || echo "not-available")
          fi

          echo "alb_dns=$ALB_DNS" >> $GITHUB_OUTPUT
          echo "ALB DNS: $ALB_DNS"

      - name: Health checks
        run: |
          ALB_DNS="${{ steps.get-alb.outputs.alb_dns }}"

          if [ "$ALB_DNS" = "not-available" ]; then
            echo "âš ï¸ ALB DNS not available, skipping health checks"
            exit 0
          fi

          ALB_URL="http://$ALB_DNS"
          echo "ðŸ¥ Running health checks against $ALB_URL..."

          for service in purchase query events; do
            echo "Testing /$service/health..."
            if curl -f -s -m 10 "$ALB_URL/$service/health" > /dev/null 2>&1; then
              echo "âœ… $service is healthy"
            else
              echo "âš ï¸ $service health check failed (may still be starting up)"
            fi
          done

      - name: Deployment Summary
        run: |
          echo "### âœ… Services Deployed Successfully!" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Image Tag:** \`${{ github.sha }}\`" >> $GITHUB_STEP_SUMMARY
          echo "**Services Updated:**" >> $GITHUB_STEP_SUMMARY
          echo "- purchase-service" >> $GITHUB_STEP_SUMMARY
          echo "- query-service" >> $GITHUB_STEP_SUMMARY
          echo "- mq-projection-service" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          ALB_DNS="${{ steps.get-alb.outputs.alb_dns }}"
          if [ "$ALB_DNS" != "not-available" ]; then
            echo "**ALB URL:** http://$ALB_DNS" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "**Test Commands:**" >> $GITHUB_STEP_SUMMARY
            echo "\`\`\`bash" >> $GITHUB_STEP_SUMMARY
            echo "curl http://$ALB_DNS/purchase/health" >> $GITHUB_STEP_SUMMARY
            echo "curl http://$ALB_DNS/query/health" >> $GITHUB_STEP_SUMMARY
            echo "curl http://$ALB_DNS/events/health" >> $GITHUB_STEP_SUMMARY
            echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
          else
            echo "**Note:** ALB URL not available. Check AWS Console for the load balancer DNS name." >> $GITHUB_STEP_SUMMARY
          fi
