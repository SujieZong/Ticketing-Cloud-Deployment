name: Deploy Ticketing System

on:
  workflow_dispatch:
    inputs:
      action:
        description: "Deployment action"
        required: true
        type: choice
        options:
          - "infrastructure-only"
          - "services-only"
          - "full-deployment"
          - "destroy-infrastructure"
          - "force-cleanup"

env:
  AWS_REGION: us-west-2
  TF_VERSION: 1.6.0

jobs:
  # Job 1: Build Services
  build:
    name: Build Java Services
    runs-on: ubuntu-latest
    if: github.event.inputs.action == 'services-only' || github.event.inputs.action == 'full-deployment'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up JDK 21
        uses: actions/setup-java@v4
        with:
          java-version: "21"
          distribution: "temurin"
          cache: maven

      - name: Build PurchaseService
        working-directory: ./PurchaseService
        run: |
          echo "ðŸ”¨ Building PurchaseService..."
          mvn clean package -DskipTests

      - name: Build QueryService
        working-directory: ./QueryService
        run: |
          echo "ðŸ”¨ Building QueryService..."
          mvn clean package -DskipTests

      - name: Build RabbitCombinedConsumer
        working-directory: ./RabbitCombinedConsumer
        run: |
          echo "ðŸ”¨ Building RabbitCombinedConsumer..."
          mvn clean package -DskipTests

      - name: Run Tests
        run: |
          echo "ðŸ§ª Running unit tests..."
          cd PurchaseService && mvn test || true
          cd ../QueryService && mvn test || true
          cd ../RabbitCombinedConsumer && mvn test || true

      - name: Upload Build Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: service-jars
          path: |
            PurchaseService/target/*.jar
            QueryService/target/*.jar
            RabbitCombinedConsumer/target/*.jar
          retention-days: 1

  # Job 2: Force Cleanup (No Terraform State Required)
  force-cleanup:
    name: Force Cleanup AWS Resources
    runs-on: ubuntu-latest
    if: github.event.inputs.action == 'force-cleanup'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-session-token: ${{ secrets.AWS_SESSION_TOKEN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Run Cleanup Script
        run: |
          echo "ðŸ§¹ Running force cleanup (no Terraform state needed)..."
          chmod +x ./config/scripts/cleanup-aws-resources.sh
          ./config/scripts/cleanup-aws-resources.sh

      - name: Wait for AWS Eventual Consistency
        run: |
          echo "â³ Waiting 3 minutes for AWS to fully process deletions..."
          echo "This is necessary due to AWS eventual consistency"
          sleep 180

      - name: Verify Cleanup
        run: |
          echo "ðŸ” Verifying all resources are deleted..."
          chmod +x ./config/scripts/verify-cleanup.sh
          ./config/scripts/verify-cleanup.sh || echo "âš ï¸ Some resources may still be deleting"

      - name: Cleanup Summary
        run: |
          echo "### ðŸ§¹ Force Cleanup Complete!" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "All AWS resources have been deleted using the cleanup script." >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**â³ Wait Time:** 3 minutes (for AWS eventual consistency)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Next Steps:**" >> $GITHUB_STEP_SUMMARY
          echo "1. âœ… Cleanup verified - resources fully deleted" >> $GITHUB_STEP_SUMMARY
          echo "2. ðŸš€ You can now run 'full-deployment' immediately" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Note:** If you still get 'already exists' errors, wait 2 more minutes and try again." >> $GITHUB_STEP_SUMMARY

  # Job 3: Terraform Infrastructure
  terraform-infrastructure:
    name: Deploy Infrastructure with Terraform
    runs-on: ubuntu-latest
    if: github.event.inputs.action == 'infrastructure-only' || github.event.inputs.action == 'full-deployment' || github.event.inputs.action == 'destroy-infrastructure'

    outputs:
      ecr_registry: ${{ steps.tf-outputs.outputs.ecr_registry }}
      ecs_cluster: ${{ steps.tf-outputs.outputs.ecs_cluster }}
      alb_dns: ${{ steps.tf-outputs.outputs.alb_dns }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Clear stale caches (for AWS Learner Lab)
        continue-on-error: true
        run: |
          echo "ðŸ§¹ Note: Clear GitHub Actions caches manually if you see 'resource already exists' errors"
          echo "Go to: https://github.com/${{ github.repository }}/actions/caches"

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-session-token: ${{ secrets.AWS_SESSION_TOKEN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TF_VERSION }}
          terraform_wrapper: false

      # Restore Terraform state for destroy operations only
      - name: Restore Terraform State
        if: github.event.inputs.action == 'destroy-infrastructure'
        uses: actions/cache/restore@v3
        with:
          path: |
            config/terraform/terraform.tfstate
            config/terraform/terraform.tfstate.backup
            config/terraform/.terraform
          key: terraform-state-${{ github.ref_name }}
          restore-keys: |
            terraform-state-${{ github.ref_name }}
            terraform-state-

      - name: Create terraform.tfvars
        working-directory: ./config/terraform
        run: |
          cat > terraform.tfvars <<EOF
          aws_region     = "${{ env.AWS_REGION }}"
          aws_account_id = "${{ secrets.AWS_ACCOUNT_ID }}"
          EOF

      - name: Terraform Init
        working-directory: ./config/terraform
        run: terraform init -upgrade

      - name: Proactive Import of Existing Resources
        if: github.event.inputs.action != 'destroy-infrastructure'
        working-directory: ./config/terraform
        continue-on-error: true
        run: |
          echo "ðŸ”„ Proactively checking and importing any existing resources..."
          REGION="${{ env.AWS_REGION }}"
          
          # Import all resources that might exist (errors are ignored)
          # This runs BEFORE plan/apply to prevent "already exists" errors
          
          # ECR
          terraform import 'module.ecr["purchase-service"].aws_ecr_repository.this' purchase-service 2>/dev/null || true
          terraform import 'module.ecr["query-service"].aws_ecr_repository.this' query-service 2>/dev/null || true
          terraform import 'module.ecr["mq-projection-service"].aws_ecr_repository.this' mq-projection-service 2>/dev/null || true
          
          # ALB
          ALB_ARN=$(aws elbv2 describe-load-balancers --region $REGION --query "LoadBalancers[?LoadBalancerName=='ticketing-alb'].LoadBalancerArn" --output text 2>/dev/null || echo "")
          [ ! -z "$ALB_ARN" ] && terraform import 'module.shared_alb.aws_lb.shared' "$ALB_ARN" 2>/dev/null || true
          
          # Target Groups
          TG_PURCHASE=$(aws elbv2 describe-target-groups --region $REGION --query "TargetGroups[?TargetGroupName=='purchase-service-tg'].TargetGroupArn" --output text 2>/dev/null || echo "")
          TG_QUERY=$(aws elbv2 describe-target-groups --region $REGION --query "TargetGroups[?TargetGroupName=='query-service-tg'].TargetGroupArn" --output text 2>/dev/null || echo "")
          TG_MQ=$(aws elbv2 describe-target-groups --region $REGION --query "TargetGroups[?TargetGroupName=='mq-projection-service-tg'].TargetGroupArn" --output text 2>/dev/null || echo "")
          [ ! -z "$TG_PURCHASE" ] && terraform import 'module.shared_alb.aws_lb_target_group.services["purchase-service"]' "$TG_PURCHASE" 2>/dev/null || true
          [ ! -z "$TG_QUERY" ] && terraform import 'module.shared_alb.aws_lb_target_group.services["query-service"]' "$TG_QUERY" 2>/dev/null || true
          [ ! -z "$TG_MQ" ] && terraform import 'module.shared_alb.aws_lb_target_group.services["mq-projection-service"]' "$TG_MQ" 2>/dev/null || true
          
          # Security Groups
          ALB_SG=$(aws ec2 describe-security-groups --region $REGION --filters "Name=group-name,Values=ticketing-alb-sg" --query "SecurityGroups[0].GroupId" --output text 2>/dev/null || echo "")
          [ ! -z "$ALB_SG" ] && [ "$ALB_SG" != "None" ] && terraform import 'module.network.aws_security_group.alb_sg' "$ALB_SG" 2>/dev/null || true
          
          ECS_SG=$(aws ec2 describe-security-groups --region $REGION --filters "Name=group-name,Values=ticketing-ecs-sg" --query "SecurityGroups[0].GroupId" --output text 2>/dev/null || echo "")
          [ ! -z "$ECS_SG" ] && [ "$ECS_SG" != "None" ] && terraform import 'module.network.aws_security_group.this' "$ECS_SG" 2>/dev/null || true
          
          RDS_SG=$(aws ec2 describe-security-groups --region $REGION --filters "Name=group-name,Values=ticketing-rds-sg" --query "SecurityGroups[0].GroupId" --output text 2>/dev/null || echo "")
          [ ! -z "$RDS_SG" ] && [ "$RDS_SG" != "None" ] && terraform import 'module.network.aws_security_group.rds_sg' "$RDS_SG" 2>/dev/null || true
          
          REDIS_SG=$(aws ec2 describe-security-groups --region $REGION --filters "Name=group-name,Values=ticketing-redis-sg" --query "SecurityGroups[0].GroupId" --output text 2>/dev/null || echo "")
          [ ! -z "$REDIS_SG" ] && [ "$REDIS_SG" != "None" ] && terraform import 'module.elasticache.aws_security_group.redis_sg' "$REDIS_SG" 2>/dev/null || true
          
          # RDS Cluster
          terraform import 'module.rds.aws_rds_cluster.this' ticketing-aurora 2>/dev/null || true
          
          # RDS Instances (if any)
          terraform import 'module.rds.aws_rds_cluster_instance.this[0]' ticketing-aurora-instance-1 2>/dev/null || true
          terraform import 'module.rds.aws_rds_cluster_instance.this[1]' ticketing-aurora-instance-2 2>/dev/null || true
          
          # CloudWatch Logs
          terraform import 'module.logging["purchase-service"].aws_cloudwatch_log_group.this' /ecs/purchase-service 2>/dev/null || true
          terraform import 'module.logging["query-service"].aws_cloudwatch_log_group.this' /ecs/query-service 2>/dev/null || true
          terraform import 'module.logging["mq-projection-service"].aws_cloudwatch_log_group.this' /ecs/mq-projection-service 2>/dev/null || true
          
          # Secrets
          terraform import 'module.elasticache.aws_secretsmanager_secret.redis' ticketing-redis-credentials 2>/dev/null || true
          terraform import 'module.rds.aws_secretsmanager_secret.db' ticketing-db-credentials 2>/dev/null || true
          
          # ElastiCache
          terraform import 'module.elasticache.aws_elasticache_subnet_group.this' ticketing-cache-subnet-group 2>/dev/null || true
          terraform import 'module.elasticache.aws_elasticache_parameter_group.this' ticketing-redis-params 2>/dev/null || true
          
          # RDS
          terraform import 'module.rds.aws_db_subnet_group.default' ticketing-aurora-subnet-group 2>/dev/null || true
          terraform import 'module.rds.aws_rds_cluster_parameter_group.this' ticketing-mysql-params 2>/dev/null || true
          
          # IAM
          POLICY_ARN=$(aws iam list-policies --query "Policies[?PolicyName=='ticketing-message-messaging-access'].Arn" --output text 2>/dev/null || echo "")
          [ ! -z "$POLICY_ARN" ] && terraform import 'module.messaging.aws_iam_policy.messaging_access' "$POLICY_ARN" 2>/dev/null || true
          
          echo "âœ… Proactive import complete - proceeding with plan/apply"

      - name: Terraform Plan
        id: plan
        working-directory: ./config/terraform
        continue-on-error: true
        run: terraform plan -no-color
        if: github.event.inputs.action != 'destroy-infrastructure'

      - name: Handle Existing Resources (Auto-Import)
        if: github.event.inputs.action != 'destroy-infrastructure' && steps.plan.outcome == 'failure'
        working-directory: ./config/terraform
        run: |
          echo "âš ï¸ Plan failed - likely due to existing resources"
          echo "ðŸ”„ Attempting to import existing resources..."

          # Import script
          REGION="${{ env.AWS_REGION }}"

          # Import ECR repos
          terraform import 'module.ecr["purchase-service"].aws_ecr_repository.this' purchase-service 2>/dev/null || true
          terraform import 'module.ecr["query-service"].aws_ecr_repository.this' query-service 2>/dev/null || true
          terraform import 'module.ecr["mq-projection-service"].aws_ecr_repository.this' mq-projection-service 2>/dev/null || true

          # Import Target Groups
          TG_PURCHASE=$(aws elbv2 describe-target-groups --region $REGION --query "TargetGroups[?TargetGroupName=='purchase-service-tg'].TargetGroupArn" --output text 2>/dev/null || echo "")
          TG_QUERY=$(aws elbv2 describe-target-groups --region $REGION --query "TargetGroups[?TargetGroupName=='query-service-tg'].TargetGroupArn" --output text 2>/dev/null || echo "")
          TG_MQ=$(aws elbv2 describe-target-groups --region $REGION --query "TargetGroups[?TargetGroupName=='mq-projection-service-tg'].TargetGroupArn" --output text 2>/dev/null || echo "")

          [ ! -z "$TG_PURCHASE" ] && terraform import 'module.shared_alb.aws_lb_target_group.services["purchase-service"]' "$TG_PURCHASE" 2>/dev/null || true
          [ ! -z "$TG_QUERY" ] && terraform import 'module.shared_alb.aws_lb_target_group.services["query-service"]' "$TG_QUERY" 2>/dev/null || true
          [ ! -z "$TG_MQ" ] && terraform import 'module.shared_alb.aws_lb_target_group.services["mq-projection-service"]' "$TG_MQ" 2>/dev/null || true

          # Import ALB (Load Balancer)
          ALB_ARN=$(aws elbv2 describe-load-balancers --region $REGION --query "LoadBalancers[?LoadBalancerName=='ticketing-alb'].LoadBalancerArn" --output text 2>/dev/null || echo "")
          [ ! -z "$ALB_ARN" ] && terraform import 'module.shared_alb.aws_lb.shared' "$ALB_ARN" 2>/dev/null || true

          # Import Log Groups
          terraform import 'module.logging["purchase-service"].aws_cloudwatch_log_group.this' /ecs/purchase-service 2>/dev/null || true
          terraform import 'module.logging["query-service"].aws_cloudwatch_log_group.this' /ecs/query-service 2>/dev/null || true
          terraform import 'module.logging["mq-projection-service"].aws_cloudwatch_log_group.this' /ecs/mq-projection-service 2>/dev/null || true

          # Import Secrets
          terraform import 'module.elasticache.aws_secretsmanager_secret.redis' ticketing-redis-credentials 2>/dev/null || true
          terraform import 'module.rds.aws_secretsmanager_secret.db' ticketing-db-credentials 2>/dev/null || true

          # Import ElastiCache resources
          terraform import 'module.elasticache.aws_elasticache_subnet_group.this' ticketing-cache-subnet-group 2>/dev/null || true
          terraform import 'module.elasticache.aws_elasticache_parameter_group.this' ticketing-redis-params 2>/dev/null || true

          # Import RDS resources
          terraform import 'module.rds.aws_db_subnet_group.default' ticketing-aurora-subnet-group 2>/dev/null || true
          terraform import 'module.rds.aws_rds_cluster_parameter_group.this' ticketing-mysql-params 2>/dev/null || true

          # Import IAM Policy
          POLICY_ARN=$(aws iam list-policies --query "Policies[?PolicyName=='ticketing-message-messaging-access'].Arn" --output text 2>/dev/null || echo "")
          [ ! -z "$POLICY_ARN" ] && terraform import 'module.messaging.aws_iam_policy.messaging_access' "$POLICY_ARN" 2>/dev/null || true

          # Import Security Groups
          ALB_SG=$(aws ec2 describe-security-groups --region $REGION --filters "Name=group-name,Values=ticketing-alb-sg" --query "SecurityGroups[0].GroupId" --output text 2>/dev/null || echo "")
          [ ! -z "$ALB_SG" ] && [ "$ALB_SG" != "None" ] && terraform import 'module.network.aws_security_group.alb_sg' "$ALB_SG" 2>/dev/null || true

          ECS_SG=$(aws ec2 describe-security-groups --region $REGION --filters "Name=group-name,Values=ticketing-ecs-sg" --query "SecurityGroups[0].GroupId" --output text 2>/dev/null || echo "")
          [ ! -z "$ECS_SG" ] && [ "$ECS_SG" != "None" ] && terraform import 'module.network.aws_security_group.this' "$ECS_SG" 2>/dev/null || true

          RDS_SG=$(aws ec2 describe-security-groups --region $REGION --filters "Name=group-name,Values=ticketing-rds-sg" --query "SecurityGroups[0].GroupId" --output text 2>/dev/null || echo "")
          [ ! -z "$RDS_SG" ] && [ "$RDS_SG" != "None" ] && terraform import 'module.network.aws_security_group.rds_sg' "$RDS_SG" 2>/dev/null || true

          REDIS_SG=$(aws ec2 describe-security-groups --region $REGION --filters "Name=group-name,Values=ticketing-redis-sg" --query "SecurityGroups[0].GroupId" --output text 2>/dev/null || echo "")
          [ ! -z "$REDIS_SG" ] && [ "$REDIS_SG" != "None" ] && terraform import 'module.network.aws_security_group.redis_sg' "$REDIS_SG" 2>/dev/null || true

          echo "âœ… Import attempts complete - retrying plan..."

      - name: Terraform Plan (Retry after import)
        if: github.event.inputs.action != 'destroy-infrastructure' && steps.plan.outcome == 'failure'
        working-directory: ./config/terraform
        run: terraform plan -no-color

      - name: Terraform Apply
        id: apply
        working-directory: ./config/terraform
        continue-on-error: true
        run: terraform apply -auto-approve
        if: github.event.inputs.action != 'destroy-infrastructure'

      - name: Handle Apply Failures (Retry with targeted imports)
        if: github.event.inputs.action != 'destroy-infrastructure' && steps.apply.outcome == 'failure'
        working-directory: ./config/terraform
        run: |
          echo "âš ï¸ Apply failed - attempting comprehensive import and retry..."

          REGION="${{ env.AWS_REGION }}"

          # Comprehensive import of all possible resources
          echo "ðŸ“¦ Importing all existing resources..."

          # ECR
          for repo in purchase-service query-service mq-projection-service; do
            terraform import "module.ecr[\"$repo\"].aws_ecr_repository.this" $repo 2>/dev/null || true
          done

          # Target Groups
          for tg_name in purchase-service-tg query-service-tg mq-projection-service-tg; do
            TG_ARN=$(aws elbv2 describe-target-groups --region $REGION --query "TargetGroups[?TargetGroupName=='$tg_name'].TargetGroupArn" --output text 2>/dev/null || echo "")
            if [ ! -z "$TG_ARN" ]; then
              service_name=$(echo $tg_name | sed 's/-tg$//')
              terraform import "module.shared_alb.aws_lb_target_group.services[\"$service_name\"]" "$TG_ARN" 2>/dev/null || true
            fi
          done

          # ALB (Load Balancer)
          ALB_ARN=$(aws elbv2 describe-load-balancers --region $REGION --query "LoadBalancers[?LoadBalancerName=='ticketing-alb'].LoadBalancerArn" --output text 2>/dev/null || echo "")
          [ ! -z "$ALB_ARN" ] && terraform import 'module.shared_alb.aws_lb.shared' "$ALB_ARN" 2>/dev/null || true

          # All other resources
          terraform import 'module.logging["purchase-service"].aws_cloudwatch_log_group.this' /ecs/purchase-service 2>/dev/null || true
          terraform import 'module.logging["query-service"].aws_cloudwatch_log_group.this' /ecs/query-service 2>/dev/null || true
          terraform import 'module.logging["mq-projection-service"].aws_cloudwatch_log_group.this' /ecs/mq-projection-service 2>/dev/null || true
          terraform import 'module.elasticache.aws_secretsmanager_secret.redis' ticketing-redis-credentials 2>/dev/null || true
          terraform import 'module.rds.aws_secretsmanager_secret.db' ticketing-db-credentials 2>/dev/null || true
          terraform import 'module.elasticache.aws_elasticache_subnet_group.this' ticketing-cache-subnet-group 2>/dev/null || true
          terraform import 'module.elasticache.aws_elasticache_parameter_group.this' ticketing-redis-params 2>/dev/null || true
          terraform import 'module.rds.aws_db_subnet_group.default' ticketing-aurora-subnet-group 2>/dev/null || true
          terraform import 'module.rds.aws_rds_cluster_parameter_group.this' ticketing-mysql-params 2>/dev/null || true

          POLICY_ARN=$(aws iam list-policies --query "Policies[?PolicyName=='ticketing-message-messaging-access'].Arn" --output text 2>/dev/null || echo "")
          [ ! -z "$POLICY_ARN" ] && terraform import 'module.messaging.aws_iam_policy.messaging_access' "$POLICY_ARN" 2>/dev/null || true

          # Security Groups (all of them)
          ALB_SG=$(aws ec2 describe-security-groups --region $REGION --filters "Name=group-name,Values=ticketing-alb-sg" --query "SecurityGroups[0].GroupId" --output text 2>/dev/null || echo "")
          [ ! -z "$ALB_SG" ] && [ "$ALB_SG" != "None" ] && terraform import 'module.network.aws_security_group.alb_sg' "$ALB_SG" 2>/dev/null || true

          ECS_SG=$(aws ec2 describe-security-groups --region $REGION --filters "Name=group-name,Values=ticketing-ecs-sg" --query "SecurityGroups[0].GroupId" --output text 2>/dev/null || echo "")
          [ ! -z "$ECS_SG" ] && [ "$ECS_SG" != "None" ] && terraform import 'module.network.aws_security_group.this' "$ECS_SG" 2>/dev/null || true

          RDS_SG=$(aws ec2 describe-security-groups --region $REGION --filters "Name=group-name,Values=ticketing-rds-sg" --query "SecurityGroups[0].GroupId" --output text 2>/dev/null || echo "")
          [ ! -z "$RDS_SG" ] && [ "$RDS_SG" != "None" ] && terraform import 'module.network.aws_security_group.rds_sg' "$RDS_SG" 2>/dev/null || true

          REDIS_SG=$(aws ec2 describe-security-groups --region $REGION --filters "Name=group-name,Values=ticketing-redis-sg" --query "SecurityGroups[0].GroupId" --output text 2>/dev/null || echo "")
          [ ! -z "$REDIS_SG" ] && [ "$REDIS_SG" != "None" ] && terraform import 'module.network.aws_security_group.redis_sg' "$REDIS_SG" 2>/dev/null || true

          echo "âœ… Import complete - retrying apply..."
          terraform apply -auto-approve

      - name: Verify Apply Success
        if: github.event.inputs.action != 'destroy-infrastructure'
        run: |
          if [ "${{ steps.apply.outcome }}" = "failure" ]; then
            echo "âŒ Terraform apply failed even after import attempts"
            echo "Check logs above for details"
            exit 1
          fi
          echo "âœ… Terraform apply successful"

      # Save state AFTER successful apply for future destroy operations
      - name: Save Terraform State
        if: github.event.inputs.action != 'destroy-infrastructure' && success()
        uses: actions/cache/save@v3
        with:
          path: |
            config/terraform/terraform.tfstate
            config/terraform/terraform.tfstate.backup
            config/terraform/.terraform
          key: terraform-state-${{ github.ref_name }}-${{ github.run_number }}

      - name: Terraform Destroy
        id: tf-destroy
        working-directory: ./config/terraform
        continue-on-error: true
        run: terraform destroy -auto-approve
        if: github.event.inputs.action == 'destroy-infrastructure'

      # Fallback: Use cleanup script if Terraform destroy fails (no state available)
      - name: Fallback - Manual Cleanup
        if: github.event.inputs.action == 'destroy-infrastructure' && steps.tf-destroy.outcome == 'failure'
        run: |
          echo "âš ï¸ Terraform destroy failed (likely no state found)"
          echo "ðŸ§¹ Running manual cleanup script instead..."
          chmod +x ./config/scripts/cleanup-aws-resources.sh
          ./config/scripts/cleanup-aws-resources.sh

      # Delete state cache after destroy
      - name: Clear State Cache After Destroy
        if: github.event.inputs.action == 'destroy-infrastructure' && success()
        continue-on-error: true
        run: |
          echo "âœ… Resources destroyed. State cache will expire naturally."
          echo "ðŸ§¹ To clear cache manually: https://github.com/${{ github.repository }}/actions/caches"

      - name: Export Infrastructure Outputs
        id: tf-outputs
        working-directory: ./config/terraform
        if: github.event.inputs.action != 'destroy-infrastructure'
        run: |
          ECR_REGISTRY=$(terraform output -raw ecr_registry 2>/dev/null || echo "not-available")
          ECS_CLUSTER=$(terraform output -raw ecs_cluster_name 2>/dev/null || echo "ticketing-prod-cluster")
          ALB_DNS=$(terraform output -raw alb_dns_name 2>/dev/null || echo "not-available")

          echo "ecr_registry=$ECR_REGISTRY" >> $GITHUB_OUTPUT
          echo "ecs_cluster=$ECS_CLUSTER" >> $GITHUB_OUTPUT
          echo "alb_dns=$ALB_DNS" >> $GITHUB_OUTPUT

      - name: Infrastructure Summary
        if: github.event.inputs.action != 'destroy-infrastructure'
        run: |
          echo "### ðŸ—ï¸ Infrastructure Deployed!" >> $GITHUB_STEP_SUMMARY
          echo "**ECS Cluster:** ${{ steps.tf-outputs.outputs.ecs_cluster }}" >> $GITHUB_STEP_SUMMARY
          echo "**ALB URL:** http://${{ steps.tf-outputs.outputs.alb_dns }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Next Step:** Run 'services-only' deployment to deploy application code" >> $GITHUB_STEP_SUMMARY

      - name: Destroy Summary
        if: github.event.inputs.action == 'destroy-infrastructure'
        run: |
          echo "### ðŸ—‘ï¸ Infrastructure Destroyed!" >> $GITHUB_STEP_SUMMARY
          echo "All AWS resources have been removed." >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Next Step:** Run 'full-deployment' to recreate infrastructure" >> $GITHUB_STEP_SUMMARY

  # Job 4: Deploy Services
  deploy-services:
    name: Build & Deploy Docker Services
    runs-on: ubuntu-latest
    needs: [build, terraform-infrastructure]
    if: |
      always() && 
      (needs.build.result == 'success' || needs.build.result == 'skipped') &&
      (needs.terraform-infrastructure.result == 'success' || needs.terraform-infrastructure.result == 'skipped') &&
      (github.event.inputs.action == 'services-only' || github.event.inputs.action == 'full-deployment')

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download Build Artifacts
        uses: actions/download-artifact@v4
        with:
          name: service-jars

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-session-token: ${{ secrets.AWS_SESSION_TOKEN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build and push PurchaseService
        env:
          ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
          IMAGE_TAG: ${{ github.sha }}
        run: |
          echo "ðŸ³ Building and pushing PurchaseService..."
          cd PurchaseService
          docker buildx build --platform linux/amd64 --push \
            -t $ECR_REGISTRY/ticketing-purchase-service:$IMAGE_TAG \
            -t $ECR_REGISTRY/ticketing-purchase-service:latest \
            .

      - name: Build and push QueryService
        env:
          ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
          IMAGE_TAG: ${{ github.sha }}
        run: |
          echo "ðŸ³ Building and pushing QueryService..."
          cd QueryService
          docker buildx build --platform linux/amd64 --push \
            -t $ECR_REGISTRY/ticketing-query-service:$IMAGE_TAG \
            -t $ECR_REGISTRY/ticketing-query-service:latest \
            .

      - name: Build and push MQ Consumer
        env:
          ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
          IMAGE_TAG: ${{ github.sha }}
        run: |
          echo "ðŸ³ Building and pushing RabbitCombinedConsumer..."
          cd RabbitCombinedConsumer
          docker buildx build --platform linux/amd64 --push \
            -t $ECR_REGISTRY/ticketing-mq-projection-service:$IMAGE_TAG \
            -t $ECR_REGISTRY/ticketing-mq-projection-service:latest \
            .

      - name: Force ECS service redeployment
        run: |
          echo "â™»ï¸ Updating ECS services with new images..."

          aws ecs update-service \
            --cluster ticketing-prod-cluster \
            --service ticketing-purchase-service \
            --force-new-deployment \
            --region ${{ env.AWS_REGION }} || echo "âš ï¸ Failed to update purchase-service (may not exist yet)"

          aws ecs update-service \
            --cluster ticketing-prod-cluster \
            --service ticketing-query-service \
            --force-new-deployment \
            --region ${{ env.AWS_REGION }} || echo "âš ï¸ Failed to update query-service (may not exist yet)"

          aws ecs update-service \
            --cluster ticketing-prod-cluster \
            --service ticketing-mq-projection-service \
            --force-new-deployment \
            --region ${{ env.AWS_REGION }} || echo "âš ï¸ Failed to update mq-projection-service (may not exist yet)"

      - name: Wait for deployment
        run: |
          echo "â³ Waiting for services to stabilize (2 minutes)..."
          sleep 120

      - name: Get ALB URL
        id: get-alb
        run: |
          # Try to get ALB DNS from Terraform output
          if [ -f "config/terraform/terraform.tfstate" ]; then
            cd config/terraform
            ALB_DNS=$(terraform output -raw alb_dns_name 2>/dev/null || echo "not-available")
          else
            # Try to get from AWS directly
            ALB_DNS=$(aws elbv2 describe-load-balancers \
              --query "LoadBalancers[?contains(LoadBalancerName, 'ticketing')].DNSName | [0]" \
              --output text 2>/dev/null || echo "not-available")
          fi

          echo "alb_dns=$ALB_DNS" >> $GITHUB_OUTPUT
          echo "ALB DNS: $ALB_DNS"

      - name: Health checks
        run: |
          ALB_DNS="${{ steps.get-alb.outputs.alb_dns }}"

          if [ "$ALB_DNS" = "not-available" ]; then
            echo "âš ï¸ ALB DNS not available, skipping health checks"
            exit 0
          fi

          ALB_URL="http://$ALB_DNS"
          echo "ðŸ¥ Running health checks against $ALB_URL..."

          for service in purchase query events; do
            echo "Testing /$service/health..."
            if curl -f -s -m 10 "$ALB_URL/$service/health" > /dev/null 2>&1; then
              echo "âœ… $service is healthy"
            else
              echo "âš ï¸ $service health check failed (may still be starting up)"
            fi
          done

      - name: Deployment Summary
        run: |
          echo "### âœ… Services Deployed Successfully!" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Image Tag:** \`${{ github.sha }}\`" >> $GITHUB_STEP_SUMMARY
          echo "**Services Updated:**" >> $GITHUB_STEP_SUMMARY
          echo "- purchase-service" >> $GITHUB_STEP_SUMMARY
          echo "- query-service" >> $GITHUB_STEP_SUMMARY
          echo "- mq-projection-service" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          ALB_DNS="${{ steps.get-alb.outputs.alb_dns }}"
          if [ "$ALB_DNS" != "not-available" ]; then
            echo "**ALB URL:** http://$ALB_DNS" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "**Test Commands:**" >> $GITHUB_STEP_SUMMARY
            echo "\`\`\`bash" >> $GITHUB_STEP_SUMMARY
            echo "curl http://$ALB_DNS/purchase/health" >> $GITHUB_STEP_SUMMARY
            echo "curl http://$ALB_DNS/query/health" >> $GITHUB_STEP_SUMMARY
            echo "curl http://$ALB_DNS/events/health" >> $GITHUB_STEP_SUMMARY
            echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
          else
            echo "**Note:** ALB URL not available. Check AWS Console for the load balancer DNS name." >> $GITHUB_STEP_SUMMARY
          fi
